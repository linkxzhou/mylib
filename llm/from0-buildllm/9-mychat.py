import gradio as gr
import torch
import os
import json
from pre_transformer import Transformer, MyPretrainConfig

try:
    from pre_configurator import llmconfig, logger
    logger.info("å·²åŠ è½½pre_configuratoré…ç½®")
except ImportError as e:
    print("æœªæ‰¾åˆ°pre_configurator.pyï¼Œå°†ä½¿ç”¨é»˜è®¤é…ç½®")
    raise e

def load_model(model_path, config_path=None):
    """åŠ è½½æ¨¡å‹å’Œé…ç½®"""
    global current_model, current_tokenizer, model_config
    
    try:
        if not os.path.exists(model_path):
            return f"æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {model_path}"
        
        # åŠ è½½é…ç½®
        if config_path and os.path.exists(config_path):
            with open(config_path, 'r', encoding='utf-8') as f:
                config_dict = json.load(f)
            model_config = MyPretrainConfig(**config_dict)
        else:
            # ä½¿ç”¨é»˜è®¤é…ç½®
            model_config = MyPretrainConfig()
        
        # åŠ è½½æ¨¡å‹
        device = torch.device('cuda' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu')
        current_model = Transformer(model_config)
        
        # åŠ è½½æƒé‡
        checkpoint = torch.load(model_path, map_location=device, weights_only=False)
        if 'model' in checkpoint:
            current_model.load_state_dict(checkpoint['model'])
        else:
            current_model.load_state_dict(checkpoint)
        
        current_model.to(device)
        current_model.eval()
        
        return f"æ¨¡å‹åŠ è½½æˆåŠŸï¼\nè®¾å¤‡: {device}\nå‚æ•°é‡: {sum(p.numel() for p in current_model.parameters()):,}"
    
    except Exception as e:
        return f"æ¨¡å‹åŠ è½½å¤±è´¥: {str(e)}"

def generate_response(message, history, max_length=512, temperature=0.7, top_p=0.9):
    """ç”Ÿæˆå¯¹è¯å›å¤"""
    global current_model, model_config
    
    if current_model is None:
        return "è¯·å…ˆåŠ è½½æ¨¡å‹ï¼"
    
    try:
        device = next(current_model.parameters()).device
        
        # ç®€å•çš„æ–‡æœ¬ç¼–ç ï¼ˆè¿™é‡Œéœ€è¦æ ¹æ®å®é™…çš„tokenizerè¿›è¡Œè°ƒæ•´ï¼‰
        # å‡è®¾ä½¿ç”¨å­—ç¬¦çº§ç¼–ç ä½œä¸ºç¤ºä¾‹
        vocab = {chr(i): i for i in range(32, 127)}  # ASCIIå¯æ‰“å°å­—ç¬¦
        vocab['<pad>'] = 0
        vocab['<unk>'] = 1
        
        def encode_text(text):
            return [vocab.get(c, vocab['<unk>']) for c in text[:max_length]]
        
        # ç¼–ç è¾“å…¥
        input_ids = encode_text(message)
        input_tensor = torch.tensor([input_ids], dtype=torch.long, device=device)
        
        # ç”Ÿæˆå›å¤
        with torch.no_grad():
            output = current_model.generate(
                input_tensor, 
                max_length=max_length,
                temperature=temperature,
                top_p=top_p
            )
        
        # è§£ç è¾“å‡ºï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼‰
        reverse_vocab = {v: k for k, v in vocab.items()}
        response = ''.join([reverse_vocab.get(token.item(), '<unk>') for token in output[0]])
        
        return response.strip()
    
    except Exception as e:
        return f"ç”Ÿæˆå›å¤æ—¶å‡ºé”™: {str(e)}"

def clear_chat():
    """æ¸…ç©ºå¯¹è¯å†å²"""
    return [], ""

# åˆ›å»º Gradio ç•Œé¢
with gr.Blocks(title="MyLLM å¯¹è¯ç³»ç»Ÿ", theme=gr.themes.Soft()) as demo:
    gr.Markdown("# ğŸ¤– MyLLM å¯¹è¯ç³»ç»Ÿ")
    gr.Markdown("ä»é›¶è®­ç»ƒçš„å¤§è¯­è¨€æ¨¡å‹å¯¹è¯ç•Œé¢")
    
    with gr.Row():
        # å·¦ä¾§æ¨¡å‹é…ç½®åŒºåŸŸ
        with gr.Column(scale=1):
            gr.Markdown("### ğŸ“ æ¨¡å‹é…ç½®")
            
            model_path = gr.Textbox(
                label="æ¨¡å‹è·¯å¾„",
                placeholder="è¾“å…¥æ¨¡å‹æ–‡ä»¶è·¯å¾„ (.pth/.pt)...",
                value="outputs/model_final.pth"
            )
            
            config_path = gr.Textbox(
                label="é…ç½®æ–‡ä»¶è·¯å¾„ï¼ˆå¯é€‰ï¼‰",
                placeholder="è¾“å…¥é…ç½®æ–‡ä»¶è·¯å¾„ (.json)...",
                value=""
            )
            
            load_btn = gr.Button("ğŸ”„ åŠ è½½æ¨¡å‹", variant="primary")
            model_status = gr.Textbox(
                label="æ¨¡å‹çŠ¶æ€",
                interactive=False,
                value="æœªåŠ è½½æ¨¡å‹"
            )
            
            gr.Markdown("### âš™ï¸ ç”Ÿæˆå‚æ•°")
            
            max_length = gr.Slider(
                minimum=50,
                maximum=2048,
                value=512,
                step=50,
                label="æœ€å¤§ç”Ÿæˆé•¿åº¦"
            )
            
            temperature = gr.Slider(
                minimum=0.1,
                maximum=2.0,
                value=0.7,
                step=0.1,
                label="æ¸©åº¦ï¼ˆåˆ›é€ æ€§ï¼‰"
            )
            
            top_p = gr.Slider(
                minimum=0.1,
                maximum=1.0,
                value=0.9,
                step=0.05,
                label="Top-pï¼ˆå¤šæ ·æ€§ï¼‰"
            )
        
        # å³ä¾§å¯¹è¯åŒºåŸŸ
        with gr.Column(scale=2):
            gr.Markdown("### ğŸ’¬ å¯¹è¯ç•Œé¢")
            
            chatbot = gr.Chatbot(
                height=500,
                label="å¯¹è¯å†å²",
                show_label=False,
                bubble_full_width=False
            )
            
            with gr.Row():
                msg = gr.Textbox(
                    label="è¾“å…¥æ¶ˆæ¯",
                    placeholder="åœ¨è¿™é‡Œè¾“å…¥æ‚¨çš„æ¶ˆæ¯...",
                    scale=4,
                    show_label=False
                )
                
                send_btn = gr.Button("ğŸ“¤ å‘é€", scale=1, variant="primary")
            
            with gr.Row():
                clear_btn = gr.Button("ğŸ—‘ï¸ æ¸…ç©ºå¯¹è¯", variant="secondary")
                
            # ç¤ºä¾‹é—®é¢˜
            gr.Markdown("### ğŸ’¡ ç¤ºä¾‹é—®é¢˜")
            example_questions = [
                "ä½ å¥½ï¼Œè¯·ä»‹ç»ä¸€ä¸‹è‡ªå·±",
                "ä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½ï¼Ÿ",
                "è¯·å†™ä¸€é¦–å…³äºæ˜¥å¤©çš„è¯—",
                "è§£é‡Šä¸€ä¸‹æœºå™¨å­¦ä¹ çš„åŸºæœ¬æ¦‚å¿µ"
            ]
            
            for question in example_questions:
                gr.Button(question, size="sm").click(
                    lambda q=question: q, outputs=msg
                )
    
    # äº‹ä»¶ç»‘å®š
    def respond(message, history, max_len, temp, top_p_val):
        if not message.strip():
            return history, ""
        
        # æ·»åŠ ç”¨æˆ·æ¶ˆæ¯åˆ°å†å²
        history.append([message, None])
        
        # ç”Ÿæˆå›å¤
        bot_response = generate_response(message, history, max_len, temp, top_p_val)
        
        # æ·»åŠ æœºå™¨äººå›å¤åˆ°å†å²
        history[-1][1] = bot_response
        
        return history, ""
    
    # ç»‘å®šäº‹ä»¶
    load_btn.click(
        load_model,
        inputs=[model_path, config_path],
        outputs=model_status
    )
    
    send_btn.click(
        respond,
        inputs=[msg, chatbot, max_length, temperature, top_p],
        outputs=[chatbot, msg]
    )
    
    msg.submit(
        respond,
        inputs=[msg, chatbot, max_length, temperature, top_p],
        outputs=[chatbot, msg]
    )
    
    clear_btn.click(
        clear_chat,
        outputs=[chatbot, msg]
    )

if __name__ == "__main__":
    # å¯åŠ¨åº”ç”¨
    demo.launch(
        server_name="0.0.0.0",
        server_port=7860,
        share=False,
        show_error=True
    )