"""
pre_moe.py - æ··åˆä¸“å®¶æ¨¡å‹ (Mixture of Experts) å®ç°

æœ¬æ–‡ä»¶å®ç°äº† MOE ç›¸å…³çš„æ ¸å¿ƒç»„ä»¶ï¼š

æ ¸å¿ƒç»„ä»¶ï¼š
- MoEGate: ä¸“å®¶é€‰æ‹©çš„é—¨æ§æœºåˆ¶ï¼Œå®ç° top-k ä¸“å®¶é€‰æ‹©å’Œè¾…åŠ©æŸå¤±è®¡ç®—
- MOEFeedForward: æ··åˆä¸“å®¶å‰é¦ˆç½‘ç»œï¼Œé›†æˆå¤šä¸ªä¸“å®¶å’Œé—¨æ§æœºåˆ¶
- validate_moe_config: MOE é…ç½®å‚æ•°éªŒè¯å‡½æ•°

å…³é”®ç‰¹æ€§ï¼š
- Top-k ä¸“å®¶é€‰æ‹©: ä¸ºæ¯ä¸ª token é€‰æ‹©æœ€ä¼˜çš„ k ä¸ªä¸“å®¶
- è¾…åŠ©æŸå¤±è®¡ç®—: å¹³è¡¡ä¸“å®¶è´Ÿè½½ï¼Œé˜²æ­¢ä¸“å®¶åˆ©ç”¨ä¸å‡
- å…±äº«ä¸“å®¶æ”¯æŒ: å¯é€‰çš„å…±äº«ä¸“å®¶æœºåˆ¶
- è®­ç»ƒ/æ¨ç†æ¨¡å¼ä¼˜åŒ–: ä¸åŒæ¨¡å¼ä¸‹çš„æ€§èƒ½ä¼˜åŒ–

ä½¿ç”¨ç¤ºä¾‹ï¼š
    from pre_moe import MOEFeedForward, validate_moe_config
    
    # éªŒè¯ MOE é…ç½®
    validate_moe_config(config)
    
    # åˆ›å»º MOE å‰é¦ˆç½‘ç»œ
    moe_ffn = MOEFeedForward(config)
    output, aux_loss = moe_ffn(input_tensor)
"""

import math
import torch
from torch import nn
import torch.nn.functional as F

def validate_moe_config(config) -> None:
    """éªŒè¯ MOE ç›¸å…³é…ç½®å‚æ•°çš„æœ‰æ•ˆæ€§ã€‚
    
    Args:
        config: æ¨¡å‹é…ç½®å¯¹è±¡
        
    Raises:
        ValueError: å½“ MOE é…ç½®å‚æ•°æ— æ•ˆæ—¶æŠ›å‡ºå¼‚å¸¸
    """
    if not config.use_moe:
        return
        
    if config.num_experts_per_tok <= 0:
        raise ValueError(f"num_experts_per_tok å¿…é¡»å¤§äº 0ï¼Œå½“å‰å€¼: {config.num_experts_per_tok}")
    
    if config.n_routed_experts <= 0:
        raise ValueError(f"n_routed_experts å¿…é¡»å¤§äº 0ï¼Œå½“å‰å€¼: {config.n_routed_experts}")
    
    if config.num_experts_per_tok > config.n_routed_experts:
        raise ValueError(f"num_experts_per_tok ({config.num_experts_per_tok}) ä¸èƒ½å¤§äº n_routed_experts ({config.n_routed_experts})")
    
    if config.n_shared_experts is not None and config.n_shared_experts < 0:
        raise ValueError(f"n_shared_experts å¿…é¡»å¤§äºç­‰äº 0 æˆ–ä¸º Noneï¼Œå½“å‰å€¼: {config.n_shared_experts}")
    
    if config.scoring_func not in ['softmax']:
        raise ValueError(f"ä¸æ”¯æŒçš„è¯„åˆ†å‡½æ•°: {config.scoring_func}ï¼Œæ”¯æŒçš„å‡½æ•°: ['softmax']")
    
    if not (0.0 <= config.aux_loss_alpha <= 1.0):
        raise ValueError(f"aux_loss_alpha å¿…é¡»åœ¨ [0.0, 1.0] èŒƒå›´å†…ï¼Œå½“å‰å€¼: {config.aux_loss_alpha}")
    
    # éªŒè¯ä¸“å®¶æ•°é‡çš„åˆç†æ€§
    if config.n_routed_experts > 64:
        print(f"[WARNING] ä¸“å®¶æ•°é‡ ({config.n_routed_experts}) å¾ˆå¤§ï¼Œå¯èƒ½å½±å“è®­ç»ƒæ•ˆç‡")
    
    if config.num_experts_per_tok == 1:
        print(f"[WARNING] æ¯ä¸ªtokenåªé€‰æ‹©1ä¸ªä¸“å®¶ï¼ŒMOEçš„æ•ˆæœå¯èƒ½æœ‰é™")

class MoEGate(nn.Module):
    """æ··åˆä¸“å®¶æ¨¡å‹çš„é—¨æ§æœºåˆ¶ã€‚
    
    å®ç° top-k ä¸“å®¶é€‰æ‹©å’Œè¾…åŠ©æŸå¤±è®¡ç®—ï¼Œç”¨äºå¹³è¡¡ä¸“å®¶è´Ÿè½½ã€‚
    """
    
    def __init__(self, config):
        super().__init__()
        self.config = config
        self.top_k = config.num_experts_per_tok
        self.n_routed_experts = config.n_routed_experts

        self.scoring_func = config.scoring_func
        self.alpha = config.aux_loss_alpha
        self.seq_aux = config.seq_aux

        # topk selection algorithm
        self.norm_topk_prob = config.norm_topk_prob
        self.gating_dim = config.dim
        self.weight = nn.Parameter(torch.empty((self.n_routed_experts, self.gating_dim)))
        self.reset_parameters()

    def reset_parameters(self) -> None:
        """é‡ç½®é—¨æ§ç½‘ç»œå‚æ•°ã€‚"""
        import torch.nn.init as init
        init.kaiming_uniform_(self.weight, a=math.sqrt(5))

    def forward(self, hidden_states):
        """å‰å‘ä¼ æ’­ï¼Œè®¡ç®—ä¸“å®¶é€‰æ‹©å’Œè¾…åŠ©æŸå¤±ã€‚
        
        Args:
            hidden_states: è¾“å…¥éšè—çŠ¶æ€ [batch_size, seq_len, hidden_dim]
            
        Returns:
            tuple: (topk_idx, topk_weight, aux_loss)
                - topk_idx: é€‰ä¸­çš„ä¸“å®¶ç´¢å¼•
                - topk_weight: ä¸“å®¶æƒé‡
                - aux_loss: è¾…åŠ©æŸå¤±ï¼ˆè®­ç»ƒæ—¶ï¼‰
        """
        bsz, seq_len, h = hidden_states.shape
        
        ### compute gating score
        hidden_states = hidden_states.reshape(-1, h)
        logits = F.linear(hidden_states, self.weight, None)
        if self.scoring_func == 'softmax':
            scores = logits.softmax(dim=-1)
        else:
            raise NotImplementedError(f'insupportable scoring function for MoE gating: {self.scoring_func}')

        ### select top-k experts
        topk_weight, topk_idx = torch.topk(scores, k=self.top_k, dim=-1, sorted=False)

        ### norm gate to sum 1
        if self.top_k > 1 and self.norm_topk_prob:
            denominator = topk_weight.sum(dim=-1, keepdim=True) + 1e-20
            topk_weight = topk_weight / denominator

        ### expert-level computation auxiliary loss
        if self.training and self.alpha > 0.0:
            scores_for_aux = scores
            aux_topk = self.top_k
            # always compute aux loss based on the naive greedy topk method
            topk_idx_for_aux_loss = topk_idx.reshape(bsz, -1)
            if self.seq_aux:
                scores_for_seq_aux = scores_for_aux.reshape(bsz, seq_len, -1)
                ce = torch.zeros(bsz, self.n_routed_experts, device=hidden_states.device)
                ce.scatter_add_(1, topk_idx_for_aux_loss,
                                torch.ones(bsz, seq_len * aux_topk, device=hidden_states.device)).div_(
                    seq_len * aux_topk / self.n_routed_experts)
                aux_loss = (ce * scores_for_seq_aux.mean(dim=1)).sum(dim=1).mean() * self.alpha
            else:
                mask_ce = F.one_hot(topk_idx_for_aux_loss.reshape(-1), num_classes=self.n_routed_experts)
                ce = mask_ce.float().mean(0)
                Pi = scores_for_aux.mean(0)
                fi = ce * self.n_routed_experts
                aux_loss = (Pi * fi).sum() * self.alpha
        else:
            aux_loss = None
        return topk_idx, topk_weight, aux_loss

class MOEFeedForward(nn.Module):
    """æ··åˆä¸“å®¶å‰é¦ˆç½‘ç»œã€‚
    
    é›†æˆå¤šä¸ªä¸“å®¶ç½‘ç»œå’Œé—¨æ§æœºåˆ¶ï¼Œæ”¯æŒè®­ç»ƒå’Œæ¨ç†æ¨¡å¼çš„ä¼˜åŒ–ã€‚
    """
    
    def __init__(self, config):
        super().__init__()
        self.config = config
        
        # å»¶è¿Ÿå¯¼å…¥ä»¥é¿å…å¾ªç¯å¯¼å…¥
        from pre_transformer import FeedForward
        
        self.experts = nn.ModuleList([
            FeedForward(
                dim=config.dim,
                hidden_dim=config.hidden_dim,
                multiple_of=config.multiple_of,
                dropout=config.dropout,
            )
            for _ in range(config.n_routed_experts)
        ])

        self.gate = MoEGate(config)
        if config.n_shared_experts is not None:
            self.shared_experts = FeedForward(
                dim=config.dim,
                hidden_dim=config.hidden_dim,
                multiple_of=config.multiple_of,
                dropout=config.dropout,
            )

    def forward(self, x):
        """å‰å‘ä¼ æ’­ã€‚
        
        Args:
            x: è¾“å…¥å¼ é‡ [batch_size, seq_len, hidden_dim]
            
        Returns:
            tuple: (output, aux_loss)
                - output: è¾“å‡ºå¼ é‡
                - aux_loss: è¾…åŠ©æŸå¤±ï¼ˆè®­ç»ƒæ—¶ï¼‰
        """
        identity = x
        orig_shape = x.shape

        # ä½¿ç”¨é—¨æ§æœºåˆ¶é€‰æ‹©ä¸“å®¶
        topk_idx, topk_weight, aux_loss = self.gate(x)

        x = x.reshape(-1, x.shape[-1])
        flat_topk_idx = topk_idx.reshape(-1)

        if self.training:
            # è®­ç»ƒæ¨¡å¼ä¸‹ï¼Œé‡å¤è¾“å…¥æ•°æ®
            x = x.repeat_interleave(self.config.num_experts_per_tok, dim=0)
            # ä¿®å¤ï¼šä½¿ç”¨ä¸è¾“å…¥ç›¸åŒçš„æ•°æ®ç±»å‹ï¼Œè€Œä¸æ˜¯ç¡¬ç¼–ç  torch.float16
            y = torch.empty_like(x, dtype=x.dtype)
            for i, expert in enumerate(self.experts):
                y[flat_topk_idx == i] = expert(x[flat_topk_idx == i])
            y = (y.reshape(*topk_weight.shape, -1) * topk_weight.unsqueeze(-1)).sum(dim=1)
            y = y.reshape(*orig_shape)
        else:
            # æ¨ç†æ¨¡å¼ä¸‹ï¼Œåªé€‰æ‹©æœ€ä¼˜ä¸“å®¶
            y = self.moe_infer(x, flat_topk_idx, topk_weight.reshape(-1, 1)).reshape(*orig_shape)

        if self.config.n_shared_experts is not None:
            y = y + self.shared_experts(identity)

        return y, aux_loss

    def moe_infer(self, x, flat_expert_indices, flat_expert_weights):
        """æ¨ç†æ¨¡å¼ä¸‹çš„ MOE è®¡ç®—ã€‚
        
        Args:
            x: è¾“å…¥å¼ é‡ [batch_seq_len, hidden_dim]
            flat_expert_indices: æ‰å¹³åŒ–çš„ä¸“å®¶ç´¢å¼• [batch_seq_len * num_experts_per_tok]
            flat_expert_weights: æ‰å¹³åŒ–çš„ä¸“å®¶æƒé‡ [batch_seq_len * num_experts_per_tok, 1]
            
        Returns:
            torch.Tensor: è¾“å‡ºå¼ é‡
        """
        batch_seq_len, hidden_dim = x.shape
        num_experts_per_tok = self.config.num_experts_per_tok
        
        # é‡å¤è¾“å…¥ä»¥åŒ¹é…ä¸“å®¶ç´¢å¼•çš„å½¢çŠ¶
        x_repeated = x.repeat_interleave(num_experts_per_tok, dim=0)
        
        y = torch.zeros_like(x_repeated)
        flat_expert_weights = flat_expert_weights.squeeze(-1)  # ç§»é™¤æœ€åä¸€ä¸ªç»´åº¦
        
        for i, expert in enumerate(self.experts):
            mask = (flat_expert_indices == i)
            if mask.any():
                selected_x = x_repeated[mask]
                if selected_x.numel() > 0:
                    expert_output = expert(selected_x)
                    y[mask] = flat_expert_weights[mask].unsqueeze(-1) * expert_output
        
        # å°†ç»“æœé‡æ–°ç»„ç»‡å¹¶æ±‚å’Œ
        y = y.view(batch_seq_len, num_experts_per_tok, hidden_dim).sum(dim=1)
        
        return y

if __name__ == "__main__":
    """æµ‹è¯• MOE ç»„ä»¶çš„å„é¡¹åŠŸèƒ½ã€‚"""
    print("=" * 60)
    print("æµ‹è¯• pre_moe.py ä¸­çš„æ··åˆä¸“å®¶æ¨¡å‹ç»„ä»¶")
    print("=" * 60)
    
    # è®¾ç½®éšæœºç§å­ä»¥ç¡®ä¿ç»“æœå¯é‡ç°
    torch.manual_seed(42)
    
    # åˆ›å»ºæµ‹è¯•é…ç½®
    class TestConfig:
        def __init__(self):
            # åŸºç¡€é…ç½®
            self.dim = 512
            self.hidden_dim = 2048
            self.multiple_of = 256
            self.dropout = 0.1
            
            # MOE é…ç½®
            self.use_moe = True
            self.num_experts_per_tok = 2
            self.n_routed_experts = 8
            self.n_shared_experts = None
            self.scoring_func = 'softmax'
            self.aux_loss_alpha = 0.01
            self.seq_aux = True
            self.norm_topk_prob = True
    
    config = TestConfig()
    
    # æµ‹è¯•å‚æ•°
    batch_size = 2
    seq_len = 16
    
    print(f"æµ‹è¯•é…ç½®:")
    print(f"  - æ¨¡å‹ç»´åº¦: {config.dim}")
    print(f"  - éšè—å±‚ç»´åº¦: {config.hidden_dim}")
    print(f"  - ä¸“å®¶æ€»æ•°: {config.n_routed_experts}")
    print(f"  - æ¯tokené€‰æ‹©ä¸“å®¶æ•°: {config.num_experts_per_tok}")
    print(f"  - è¾…åŠ©æŸå¤±æƒé‡: {config.aux_loss_alpha}")
    print(f"  - åºåˆ—é•¿åº¦: {seq_len}")
    print(f"  - æ‰¹æ¬¡å¤§å°: {batch_size}")
    print()
    
    # 1. æµ‹è¯• MOE é…ç½®éªŒè¯
    print("1. æµ‹è¯• MOE é…ç½®éªŒè¯ (validate_moe_config)")
    print("-" * 40)
    try:
        validate_moe_config(config)
        print("âœ“ MOE é…ç½®éªŒè¯é€šè¿‡")
        
        # æµ‹è¯•æ— æ•ˆé…ç½®
        invalid_config = TestConfig()
        invalid_config.num_experts_per_tok = 0
        try:
            validate_moe_config(invalid_config)
            print("âœ— åº”è¯¥æ£€æµ‹åˆ°æ— æ•ˆé…ç½®")
        except ValueError as e:
            print(f"âœ“ æ­£ç¡®æ£€æµ‹åˆ°æ— æ•ˆé…ç½®: {e}")
            
    except Exception as e:
        print(f"âœ— MOE é…ç½®éªŒè¯æµ‹è¯•å¤±è´¥: {e}")
        raise e
        
    print()
    
    # 2. æµ‹è¯• MoEGate é—¨æ§æœºåˆ¶
    print("2. æµ‹è¯• MoEGate é—¨æ§æœºåˆ¶")
    print("-" * 40)
    try:
        gate = MoEGate(config)
        
        # åˆ›å»ºæµ‹è¯•è¾“å…¥
        x = torch.randn(batch_size, seq_len, config.dim)
        print(f"è¾“å…¥å½¢çŠ¶: {x.shape}")
        
        # å‰å‘ä¼ æ’­
        topk_idx, topk_weight, aux_loss = gate(x)
        
        print(f"âœ“ Top-k ä¸“å®¶ç´¢å¼•å½¢çŠ¶: {topk_idx.shape}")
        print(f"âœ“ Top-k ä¸“å®¶æƒé‡å½¢çŠ¶: {topk_weight.shape}")
        print(f"âœ“ è¾…åŠ©æŸå¤±: {aux_loss.item() if aux_loss is not None else None}")
        
        # éªŒè¯ä¸“å®¶é€‰æ‹©çš„æœ‰æ•ˆæ€§
        assert topk_idx.shape == (batch_size * seq_len, config.num_experts_per_tok)
        assert topk_weight.shape == (batch_size * seq_len, config.num_experts_per_tok)
        assert torch.all(topk_idx >= 0) and torch.all(topk_idx < config.n_routed_experts)
        assert torch.all(topk_weight >= 0) and torch.all(topk_weight <= 1)
        
        # æ£€æŸ¥æƒé‡å½’ä¸€åŒ–
        if config.norm_topk_prob and config.num_experts_per_tok > 1:
            weight_sums = topk_weight.sum(dim=-1)
            assert torch.allclose(weight_sums, torch.ones_like(weight_sums), atol=1e-6)
            print("âœ“ æƒé‡å½’ä¸€åŒ–æ­£ç¡®")
        
        print("âœ“ MoEGate é—¨æ§æœºåˆ¶æµ‹è¯•é€šè¿‡")
        
    except Exception as e:
        print(f"âœ— MoEGate é—¨æ§æœºåˆ¶æµ‹è¯•å¤±è´¥: {e}")
        raise e

    print()
    
    # 3. æµ‹è¯• MOEFeedForward å‰é¦ˆç½‘ç»œ
    print("3. æµ‹è¯• MOEFeedForward å‰é¦ˆç½‘ç»œ")
    print("-" * 40)
    try:
        moe_ffn = MOEFeedForward(config)
        
        # åˆ›å»ºæµ‹è¯•è¾“å…¥
        x = torch.randn(batch_size, seq_len, config.dim)
        print(f"è¾“å…¥å½¢çŠ¶: {x.shape}")
        
        # è®­ç»ƒæ¨¡å¼æµ‹è¯•
        moe_ffn.train()
        output_train, aux_loss_train = moe_ffn(x)
        
        print(f"âœ“ è®­ç»ƒæ¨¡å¼è¾“å‡ºå½¢çŠ¶: {output_train.shape}")
        print(f"âœ“ è®­ç»ƒæ¨¡å¼è¾…åŠ©æŸå¤±: {aux_loss_train.item() if aux_loss_train is not None else None}")
        
        # æ¨ç†æ¨¡å¼æµ‹è¯•
        moe_ffn.eval()
        with torch.no_grad():
            output_eval, aux_loss_eval = moe_ffn(x)
        
        print(f"âœ“ æ¨ç†æ¨¡å¼è¾“å‡ºå½¢çŠ¶: {output_eval.shape}")
        print(f"âœ“ æ¨ç†æ¨¡å¼è¾…åŠ©æŸå¤±: {aux_loss_eval}")
        
        # éªŒè¯è¾“å‡ºå½¢çŠ¶
        assert output_train.shape == x.shape
        assert output_eval.shape == x.shape
        assert aux_loss_eval is None  # æ¨ç†æ¨¡å¼ä¸‹åº”è¯¥æ²¡æœ‰è¾…åŠ©æŸå¤±
        
        print("âœ“ MOEFeedForward å‰é¦ˆç½‘ç»œæµ‹è¯•é€šè¿‡")
        
    except Exception as e:
        print(f"âœ— MOEFeedForward å‰é¦ˆç½‘ç»œæµ‹è¯•å¤±è´¥: {e}")
        raise e

    print()
    
    # 4. æµ‹è¯•å…±äº«ä¸“å®¶åŠŸèƒ½
    print("4. æµ‹è¯•å…±äº«ä¸“å®¶åŠŸèƒ½")
    print("-" * 40)
    try:
        # åˆ›å»ºå¸¦å…±äº«ä¸“å®¶çš„é…ç½®
        shared_config = TestConfig()
        shared_config.n_shared_experts = 1
        
        validate_moe_config(shared_config)
        moe_ffn_shared = MOEFeedForward(shared_config)
        
        # æµ‹è¯•å‰å‘ä¼ æ’­
        output_shared, aux_loss_shared = moe_ffn_shared(x)
        
        print(f"âœ“ å…±äº«ä¸“å®¶è¾“å‡ºå½¢çŠ¶: {output_shared.shape}")
        print(f"âœ“ å…±äº«ä¸“å®¶è¾…åŠ©æŸå¤±: {aux_loss_shared.item() if aux_loss_shared is not None else None}")
        
        assert output_shared.shape == x.shape
        print("âœ“ å…±äº«ä¸“å®¶åŠŸèƒ½æµ‹è¯•é€šè¿‡")
        
    except Exception as e:
        print(f"âœ— å…±äº«ä¸“å®¶åŠŸèƒ½æµ‹è¯•å¤±è´¥: {e}")
    print()
    
    # 5. æµ‹è¯•ä¸“å®¶è´Ÿè½½å‡è¡¡
    print("5. æµ‹è¯•ä¸“å®¶è´Ÿè½½å‡è¡¡")
    print("-" * 40)
    try:
        # ç»Ÿè®¡ä¸“å®¶ä½¿ç”¨é¢‘ç‡
        expert_counts = torch.zeros(config.n_routed_experts)
        num_samples = 100
        
        moe_ffn.train()
        for _ in range(num_samples):
            sample_x = torch.randn(1, seq_len, config.dim)
            topk_idx, _, _ = moe_ffn.gate(sample_x)
            for idx in topk_idx.flatten():
                expert_counts[idx] += 1
        
        expert_usage = expert_counts / expert_counts.sum()
        print(f"ä¸“å®¶ä½¿ç”¨åˆ†å¸ƒ: {expert_usage.tolist()}")
        
        # æ£€æŸ¥è´Ÿè½½å‡è¡¡æ€§
        expected_usage = 1.0 / config.n_routed_experts
        max_deviation = torch.abs(expert_usage - expected_usage).max()
        print(f"âœ“ æœ€å¤§åå·®: {max_deviation:.4f}")
        print(f"âœ“ æœŸæœ›ä½¿ç”¨ç‡: {expected_usage:.4f}")
        
        if max_deviation < 0.2:  # å…è®¸20%çš„åå·®
            print("âœ“ ä¸“å®¶è´Ÿè½½ç›¸å¯¹å‡è¡¡")
        else:
            print("âš  ä¸“å®¶è´Ÿè½½ä¸å¤Ÿå‡è¡¡ï¼Œå¯èƒ½éœ€è¦è°ƒæ•´è¾…åŠ©æŸå¤±æƒé‡")
            
    except Exception as e:
        print(f"âœ— ä¸“å®¶è´Ÿè½½å‡è¡¡æµ‹è¯•å¤±è´¥: {e}")
    print()
    
    # 6. æ€§èƒ½åŸºå‡†æµ‹è¯•
    print("6. æ€§èƒ½åŸºå‡†æµ‹è¯•")
    print("-" * 40)
    try:
        import time
        
        # åˆ›å»ºæ™®é€šå‰é¦ˆç½‘ç»œä½œä¸ºå¯¹æ¯”
        from pre_transformer import FeedForward
        normal_ffn = FeedForward(
            dim=config.dim,
            hidden_dim=config.hidden_dim,
            multiple_of=config.multiple_of,
            dropout=config.dropout
        )
        
        # é¢„çƒ­
        for _ in range(5):
            _ = moe_ffn(x)
            _ = normal_ffn(x)
        
        # æµ‹è¯• MOE æ€§èƒ½
        moe_ffn.eval()
        start_time = time.time()
        num_runs = 20
        for _ in range(num_runs):
            with torch.no_grad():
                _, _ = moe_ffn(x)
        moe_time = (time.time() - start_time) / num_runs
        
        # æµ‹è¯•æ™®é€šå‰é¦ˆç½‘ç»œæ€§èƒ½
        normal_ffn.eval()
        start_time = time.time()
        for _ in range(num_runs):
            with torch.no_grad():
                _ = normal_ffn(x)
        normal_time = (time.time() - start_time) / num_runs
        
        print(f"âœ“ MOE å‰é¦ˆç½‘ç»œå¹³å‡æ—¶é—´: {moe_time*1000:.2f} ms")
        print(f"âœ“ æ™®é€šå‰é¦ˆç½‘ç»œå¹³å‡æ—¶é—´: {normal_time*1000:.2f} ms")
        print(f"âœ“ æ€§èƒ½æ¯”ç‡ (MOE/Normal): {moe_time/normal_time:.2f}x")
        
        if moe_time / normal_time < 3.0:  # MOE æ—¶é—´ä¸è¶…è¿‡æ™®é€šç½‘ç»œçš„3å€
            print("âœ“ MOE æ€§èƒ½å¼€é”€åœ¨å¯æ¥å—èŒƒå›´å†…")
        else:
            print("âš  MOE æ€§èƒ½å¼€é”€è¾ƒå¤§")
            
    except Exception as e:
        print(f"âœ— æ€§èƒ½åŸºå‡†æµ‹è¯•å¤±è´¥: {e}")
    print()
    
    # 7. æ¢¯åº¦æ£€æŸ¥
    print("7. æ¢¯åº¦æ£€æŸ¥")
    print("-" * 40)
    try:
        # è®¾ç½®éœ€è¦æ¢¯åº¦
        x_grad = torch.randn(batch_size, seq_len, config.dim, requires_grad=True)
        
        # å‰å‘ä¼ æ’­
        moe_ffn.train()
        output, aux_loss = moe_ffn(x_grad)
        
        # è®¡ç®—æ€»æŸå¤±
        main_loss = output.sum()
        total_loss = main_loss
        if aux_loss is not None:
            total_loss = main_loss + aux_loss
        
        # åå‘ä¼ æ’­
        total_loss.backward()
        
        print(f"âœ“ è¾“å…¥æ¢¯åº¦å½¢çŠ¶: {x_grad.grad.shape}")
        print(f"âœ“ è¾“å…¥æ¢¯åº¦èŒƒæ•°: {x_grad.grad.norm():.6f}")
        print(f"âœ“ ä¸»æŸå¤±: {main_loss.item():.6f}")
        if aux_loss is not None:
            print(f"âœ“ è¾…åŠ©æŸå¤±: {aux_loss.item():.6f}")
        
        # æ£€æŸ¥é—¨æ§ç½‘ç»œæ¢¯åº¦
        gate_grad_norm = moe_ffn.gate.weight.grad.norm()
        print(f"âœ“ é—¨æ§ç½‘ç»œæ¢¯åº¦èŒƒæ•°: {gate_grad_norm:.6f}")
        
        # æ£€æŸ¥ä¸“å®¶ç½‘ç»œæ¢¯åº¦
        expert_grad_norms = []
        for i, expert in enumerate(moe_ffn.experts):
            expert_grad_norm = sum(p.grad.norm().item() for p in expert.parameters() if p.grad is not None)
            expert_grad_norms.append(expert_grad_norm)
        
        print(f"âœ“ ä¸“å®¶ç½‘ç»œæ¢¯åº¦èŒƒæ•°: {expert_grad_norms}")
        print("âœ“ æ¢¯åº¦æ£€æŸ¥æµ‹è¯•é€šè¿‡")
        
    except Exception as e:
        print(f"âœ— æ¢¯åº¦æ£€æŸ¥æµ‹è¯•å¤±è´¥: {e}")
    print()
    
    # 8. ä¸åŒé…ç½®æµ‹è¯•
    print("8. ä¸åŒé…ç½®æµ‹è¯•")
    print("-" * 40)
    try:
        # æµ‹è¯•ä¸åŒçš„ä¸“å®¶é€‰æ‹©æ•°é‡
        for k in [1, 2, 4]:
            if k <= config.n_routed_experts:
                test_config = TestConfig()
                test_config.num_experts_per_tok = k
                
                validate_moe_config(test_config)
                test_moe = MOEFeedForward(test_config)
                
                output, aux_loss = test_moe(x)
                print(f"âœ“ Top-{k} ä¸“å®¶é…ç½®æµ‹è¯•é€šè¿‡ï¼Œè¾“å‡ºå½¢çŠ¶: {output.shape}")
        
        # æµ‹è¯•ä¸åŒçš„è¯„åˆ†å‡½æ•°
        print("âœ“ ä¸åŒé…ç½®æµ‹è¯•é€šè¿‡")
        
    except Exception as e:
        print(f"âœ— ä¸åŒé…ç½®æµ‹è¯•å¤±è´¥: {e}")
    print()
    
    print("=" * 60)
    print("æ‰€æœ‰æµ‹è¯•å®Œæˆï¼MOE ç»„ä»¶åŠŸèƒ½æ­£å¸¸ã€‚")
    print("=" * 60)
    
    # è¾“å‡ºæ€»ç»“ä¿¡æ¯
    print("\nğŸ“Š MOE ç»„ä»¶æ€»ç»“:")
    print(f"- ä¸“å®¶æ€»æ•°: {config.n_routed_experts}")
    print(f"- æ¯tokené€‰æ‹©ä¸“å®¶æ•°: {config.num_experts_per_tok}")
    print(f"- å‚æ•°æ€»æ•°: {sum(p.numel() for p in moe_ffn.parameters()):,}")
    print(f"- é—¨æ§ç½‘ç»œå‚æ•°: {moe_ffn.gate.weight.numel():,}")
    print(f"- å•ä¸ªä¸“å®¶å‚æ•°: {sum(p.numel() for p in moe_ffn.experts[0].parameters()):,}")
    print(f"- ä¸“å®¶ç½‘ç»œæ€»å‚æ•°: {sum(p.numel() for expert in moe_ffn.experts for p in expert.parameters()):,}")